{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_6th_sem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LekUUA4Nan4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "bc758611-8e4c-4a2b-ec83-2b4a56cebc7b"
      },
      "source": [
        "#!pip install PyDrive\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.11)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV55LnUIa9X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu7JRDjVbTns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bATRKBSUbzmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e6565eb-794c-498b-c92f-a56ff39df5b7"
      },
      "source": [
        "print (\"Downloading my file\")\n",
        "#myfile = drive.CreateFile({'id': '1-hMc3oaWlypHUObtcoLgrRVxq3XAF5p1'})\n",
        "#myfile.GetContentFile('train.txt')\n",
        "f=open('train.txt','r')\n",
        "dt=f.read(798582216)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading my file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4t8YeEifUqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "dt=re.sub(\"__label__1 \",'0bhaiyu',dt)\n",
        "dt=re.sub(\"__label__2 \",'1bhaiyu',dt)\n",
        "dt=re.sub(\"\\*\",\" \",dt)\n",
        "dt=re.sub('\\\"',\" \",dt)\n",
        "import string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPU5gXKhIbTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stopwords=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFzucVkWnv5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ecac4d6a-ff50-44f6-89d6-02924fa32a20"
      },
      "source": [
        "dt=dt.lower()\n",
        "ds=''\n",
        "for i in dt.split(\"\\n\"):\n",
        "  x=' '.join(ch for ch in i.split() if ch not in stopwords if len(ch)>2)\n",
        "  ds+=x\n",
        "  ds+=(\"\\n\")\n",
        "print(ds[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1bhaiyustuning even non-gamer: sound track beautiful! paints senery mind well would recomend even people hate vid. game music! played game chrono cross games ever played best music! backs away crude keyboarding takes fresher step grate guitars soulful orchestras. would impress anyone cares listen! ^_^\n",
            "1bhaiyuthe best soundtrack ever anything.: i'm reading lot reviews saying best 'game soundtrack' figured i'd write review disagree bit. opinino yasunori mitsuda's ultimate masterpiece. music timele\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6HIhvNGfB8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0392bce3-4f73-499a-fb7b-b2964e3bb9a1"
      },
      "source": [
        "del dt\n",
        "exclude=string.punctuation\n",
        "ds= ''.join(ch for ch in ds if ch not in exclude)\n",
        "print(ds[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1bhaiyustuning even nongamer sound track beautiful paints senery mind well would recomend even people hate vid game music played game chrono cross games ever played best music backs away crude keyboarding takes fresher step grate guitars soulful orchestras would impress anyone cares listen \n",
            "1bhaiyuthe best soundtrack ever anything im reading lot reviews saying best game soundtrack figured id write review disagree bit opinino yasunori mitsudas ultimate masterpiece music timeless im listening year\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU_xG7XRyC14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "58c4b8dc-9d46-4c32-a1fd-1453e9843b45"
      },
      "source": [
        "import pandas as pd\n",
        "a=open(r\"C:\\Users\\subham anand\\Desktop\\dataset2.txt\",\"w+\",encoding='utf-8')\n",
        "a.write(ds)\n",
        "a.close()\n",
        "df=pd.read_csv(r\"C:\\Users\\subham anand\\Desktop\\dataset2.txt\",delimiter='bhaiyu',names=['label','sentence'],engine='python', quotechar='\"', error_bad_lines=False)\n",
        "print(df.shape)\n",
        "print(df[df[\"label\"]==1].count())\n",
        "print(df[df[\"label\"]==2].count())\n",
        "print(df[:10])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1784455, 2)\n",
            "label       895486\n",
            "sentence    895485\n",
            "dtype: int64\n",
            "label       0\n",
            "sentence    0\n",
            "dtype: int64\n",
            "   label                                           sentence\n",
            "0      1  stuning even nongamer sound track beautiful pa...\n",
            "1      1  the best soundtrack ever anything im reading l...\n",
            "2      1  amazing soundtrack favorite music time hands d...\n",
            "3      1  excellent soundtrack truly like soundtrack enj...\n",
            "4      1  remember pull jaw floor hearing it youve playe...\n",
            "5      1  an absolute masterpiece quite sure actually ta...\n",
            "6      0  buyer beware selfpublished book want know whyr...\n",
            "7      1  glorious story loved whisper wicked saints sto...\n",
            "8      1  a five star book finished reading whisper wick...\n",
            "9      1  whispers wicked saints easy read book made wan...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDd-nQ2cCkDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "69f04aff-2332-4187-f332-0d4567e7a3d2"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         label                                           sentence\n",
            "0            1  stuning even nongamer sound track beautiful pa...\n",
            "1            1  the best soundtrack ever anything im reading l...\n",
            "2            1  amazing soundtrack favorite music time hands d...\n",
            "3            1  excellent soundtrack truly like soundtrack enj...\n",
            "4            1  remember pull jaw floor hearing it youve playe...\n",
            "...        ...                                                ...\n",
            "1784450      1  a sexy love story believable conflict alejandr...\n",
            "1784451      0  so many comfort foods missing hate give book s...\n",
            "1784452      1  g diet book good resource anyone follow gluten...\n",
            "1784453      1  great recipe elaborate list ingredients also b...\n",
            "1784454      0  plenty unhelpful recipes book good information...\n",
            "\n",
            "[1784455 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytPH8PeHiwhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convert_dict = {'label': int, \n",
        "                'sentence': str\n",
        "               } \n",
        "  \n",
        "df = df.astype(convert_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs--PEyEfsXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "sentences=df['sentence'].values\n",
        "y=df['label'].values\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCFD3MdSn6HM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "485e37ea-2912-4fc8-fb66-451f6282955a"
      },
      "source": [
        "print(sentences_train[100:120])\n",
        "print(y_train[100:120])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['awesome book awesome best part tells details artists inspirations really five star tintin book'\n",
            " 'hidden treasure anyone loves folk music wear got wifebut cant stop listening hooray'\n",
            " 'good idea poorly executed delphi skyfi3 radio great ideads poorly executed great features etc apparently software programmers forgot test log rhythms went problems people mentioned log ups connecting pc one annoying chirping sound sad it gave returned got money back delphi xpress rc worked right box sounds great lock up stay away skyfi3 work issues'\n",
            " 'need help finding song someone help get name song plays coach carter fired hes trunk song plays video cant remember artist name know part verse goes remember first time heard juicy big cleanin crib moms somethin like that one help please'\n",
            " 'bad book exciting story line suspenseful writing style reviewers found book totally eluded me plot farcical heavy improbable relationships unbelievable legal police procedures characters seemed driven endless supply adolescent hormones dialogue bordering offensive say much quality writing expectations readers'\n",
            " 'had high hopes terribly disappointed deserves stars listened disc first time in car way home tower records found thinking boy singers sound disconnected its almost like arent room even time zone got home read liner notes find deed case domingo cuts recorded different days skovhus cuts find terribly disappointing order songs hang together singers need really understand empathize performace thats pretty damn hard theyre even city probablythis domingo disc likely never listen used store go'\n",
            " 'an incredible premise incredible premise ancient times irish visitor egypt shown plans toy amuse pharaoh using steam power ask inventor permission draw picture works takes back ireland irish develop steam power ancient times become technology advanced create space program 1400s launch spaceship settle another planet new world alternate history irelands main allies red men natives north south america though steampunk elements sublet would definitely call steampunk epilogue incredible highly recommend book'\n",
            " 'captivating walk dread diary deliverance trembley witness salem witch trials must readers interested salem witch trials one readers excited book trials interesting time early american history mainly mysteries book solve mysteries helps understand bystander felt like bystander believe outcries witches included story sibling love and hatred times money worries friendships courtships farm life youll never want put book down its satisfying read author lisa rowe fraustino welcomed writer dear america family recommend'\n",
            " 'good one good family movie also appears though movie truth it good save whales movie'\n",
            " 'bamford caught lie even star bamfords main claim work bushs proisrael policy the rants antisemite worst kind started january 2001 main cause 911however mr bamford writing book facts antisemitic antibush slander would realize bin laden launching 911 plan 1998 1999 bill clinton office kowtowing arafat letting palestinians get away terrorism so using bamfords thesis perhaps possible believe clintons refusal anything palestinian terrorism first wtc bombing khobar towers attacks saudi arabia twin embassy attacks africa 1998 fact emboldened bin laden attack mainland united states'\n",
            " 'a good warmer product works well abide instructions put suggested amount water drop more its important keep bottom reservoir clean otherwise minerals water start collecting burn smell looks like rust wipes right out pleased performance price paid'\n",
            " 'carlotta feisty heroine concealing big surprise so okay its unusual bunch kids dare one number knock door person theyve decided definitely peculiar unusual carlottas secret happens carlotta gets courage knock miss simons doorcarlotta takes dare wants fit webster street gang wants play baseball team moved willow springs california new york city needs friendswhat happens carlotta knocks miss simons door big surprise bigger surprise carlotta handles it wait webster street gang finds carlottas secret really is'\n",
            " 'for beginner natural healthnatural childbirth midwife recommended book proved favorite book pregnancy easy read offered advice many issues pregnancy childbirth fertility issues infant problems new herbal medicine found weeds guidance easy follow book along raspberry leaf tea would make great gift pregnant friends'\n",
            " 'please stop book light years ahead pillars creation far quality would much better stand alone book without richard kahlan could new world story centered around minor characters such nathan verna would taken small changes make stand alone book dont see another villain week little tired first pages reminding arethen bad guy comes nowhere causes problems hero dispached maenwhile guess emporor jagang waiting miles away eating food drinking beer waiting end seriesthese rapidly turning pulp books resemble serial much epic sagaplease take years write good book mr goodkind know it'\n",
            " 'finally book read again greatest book ever written content rivals bible story sheer genius cant wait movie comes out hope potray characters exactly way love book wish schools would start making required textbook instead evilution books make mockery belief christians buy book change life'\n",
            " 'why bother publishing kinds books admit book really bad seems likethe author put much time reasearch project quite disappointed product probably last book ever buy que examples work netscape 30 waste money'\n",
            " 'to predictable read digital fortess deception point reading digital fortess deception absolutely fun plot easily predicted whenever author doesnt tell something trying trick you knew book going end even half way done it'\n",
            " 'i wish rating system zero star option excited movie great start then first murder tension filled shocking that whole thing fell apart kept trying figure going end asked myself is central idea movie many possibilities feel wasted impressed'\n",
            " 'bud lou classics again classicbud lou alaska find friends goldmindthis funny routinessome bud lou fish lou pulls bud water lou hey tom see fish water bud slaps lou'\n",
            " 'boringoh boring saw stupidest movie ever seenall hype one scareill tell thoughif like hear pepole use word againgo watch otherwise pointless movie']\n",
            "[1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp6hDw07hmNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.utils import np_utils\n",
        "#for  i in range(len(y_train)):\n",
        "#  y_train[i] = np_utils.to_categorical(y_train[i], 2)\n",
        "#y_test = np_utils.to_categorical(y_test,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anB6POy9f7zL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6415d493-c85d-4d17-bd5d-acc2b0f78a60"
      },
      "source": [
        "sentences_train_processed=[]\n",
        "for i in sentences_train:\n",
        "  sentences_train_processed.append(str(i).split())\n",
        "print(sentences_train_processed[:5])\n",
        "\n",
        "sentences_test_processed=[]\n",
        "for i in sentences_test:\n",
        "  sentences_test_processed.append(str(i).split())\n",
        "print(sentences_test_processed[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['great', 'family', 'camping', 'great', 'family', 'camping', 'trips', 'afford', 'weight', 'batteries', 'durable', 'piece', 'equipment', 'decent', 'battery', 'life', 'kids', 'afraid', 'dark', 'want', 'light', 'tent', 'good', 'product'], ['defective', 'pulling', 'tabs', 'would', 'like', 'relate', 'recent', 'experience', 'ding', 'king', 'promise', 'update', 'new', 'developments', 'occurmy', 'order', 'arrived', 'read', 'instructions', 'attempted', 'repair', 'glue', 'would', 'bond', 'pulling', 'tab', 'called', 'provided', 'support', 'number', 'told', 'perhaps', 'used', 'much', 'glue', 'tried', 'using', 'less', 'glue', 'made', 'difference', 'decided', 'return', 'product', 'told', 'potters', 'gifts', 'since', 'product', 'longer', 'like', 'new', 'condition', 'ineligible', 'refund', 'anger', 'called', 'support', 'number', 'let', 'know', 'intended', 'let', 'amazon', 'community', 'know', 'products', 'poor', 'performance', 'unjust', 'treatment', 'time', 'told', 'batch', 'ding', 'king', 'product', 'sent', 'defective', 'plastic', 'adhere', 'pulling', 'tabs', 'gave', 'number', '800', '4716123', 'call', 'remedy', 'called', 'sending', 'pulling', 'tabsi', 'post', 'receive', 'them'], ['this', 'best', 'ever', 'anyone', 'likes', 'spongebob', 'great', 'its', 'got', 'songs', 'favorite', 'little', 'square', 'dude', 'awesome', 'friends', 'spongebob', 'rules'], ['a', 'good', 'read', 'cargo', 'dispatch', 'flush', 'it', 'book', 'would', 'great', 'resource', 'aspiring', 'mystery', 'suspense', 'porn', 'writers', 'stages', 'rigor', 'mortis', 'squeamish', 'curiosities', 'its', 'covered', 'uncovered', 'case', 'may', 'be', 'items', 'appropriate', 'nonadults', 'excerpts', 'describing', 'photos', 'madonnas', 'sex', 'book', 'excerpts', 'describing', 'graphic', 'murders', 'american', 'psycho', 'theres', 'reason', 'people', 'avoid', 'items', 'choice', 'reference', 'amusement', 'purposes', 'only', 'warned', 'items', 'warrant', 'skipping', 'retain', 'lunch', 'received', 'joke', 'gift', 'selfchosen', 'purchase'], ['prom', 'nightwould', 'prom', 'nights', 'misery', 'cool', 'think', 'theme', 'anthos', 'another', 'collection', 'around', 'tired', 'cliche', 'prom', 'night', 'least', 'mainstream', 'american', 'culture', 'screams', 'carrie', 'prom', 'night', 'total', 'surprisereading', 'it', 'jealous', 'hadnt', 'wonderfully', 'chaotic', 'sad', 'experience', 'led', 'pmespecially', 'lorelei', 'shannons', 'ironic', 'wildly', 'humorous', 'look', 'whos', 'real', 'dog', 'local', 'high', 'school', 'peggy', 'sue', 'got', 'slobbered', 'fred', 'saberhagens', 'hauntingly', 'sad', 'senior', 'prom', 'sex', 'thing', 'keeps', 'safe', 'rest', 'life', 'ar', 'morlans', 'mesmerizing', 'changing', 'colors', 'language', 'dress', 'short', 'story', 'shorter', 'miniskirt', 'ten', 'times', 'sexythe', 'writing', 'topnotch', 'viewpoints', 'horrifyingcreative', 'always', 'surprise', 'ill', 'buy', 'next', 'novel', 'anyone', 'collection', 'puts', 'out']]\n",
            "[['sound', 'fair', 'beat', 'right', 'on', 'released', 'vinyl', 'seeco', 'records', 'sclp', '9082', 'little', 'remembered', 'orchestra', 'leader', 'puerto', 'rico', 'deserves', 'work', 'musicians', 'widely', 'recognized', 'dance', 'recordings', '1950s', 'worth', 'hearing', 'todayprogram', 'regreso', 'walter', 'winchell', 'rhumba', 'noro', 'rumbaland', 'rumbambola', 'echa', 'pa', 'vamos', 'dame', 'cacho', 'empezo', 'stop', 'serenata', 'ritmica', 'mambo', 'reina'], ['not', 'good', 'agree', 'awful', 'dulli', 'dont', 'think', 'man', 'produced', 'movie', 'ever', 'read', 'true', 'bible', 'story', 'david', 'goliath', 'good', 'movie', 'want', 'sit', 'watch', 'old', 'movie', 'popcorn', 'past', 'time'], ['satisfactory', 'phone', 'couple', 'trials', 'instead', 'going', 'panasonic', 'though', 'save', 'money', 'went', 'phones', 'vtechmotorollaas', 'great', 'deal', 'them', 'move', 'worked', 'well', 'poor', 'sound', 'quality', 'display', 'skewed', 'weak', 'battery', 'finally', 'year', 'half', 'phone', 'usage', 'got', 'panasonic', 'phonei', 'pretty', 'much', 'satisfied', 'one', 'good', 'sound', 'units', 'liked', 'key', 'pad', 'toosoft', 'big', 'keys', 'good', 'intercom', 'feature', 'may', 'dont', 'use', 'though', 'unit', 'always', 'prefered', 'call', 'cell', 'phone', 'love', 'calling', 'receiving', 'phones', 'unit'], ['not', 'golden', 'tee', 'wrote', 'review', 'game', 'targetcom', 'wanted', 'add', 'one', 'note', 'review', 'moreso', 'image', 'see', 'ad', 'look', 'screen', 'tv', 'not', 'repeat', 'not', 'game', 'looks', 'like', 'contacted', 'targetcom', 'let', 'know', 'false', 'advertising', 'returned', 'shipping', 'cost', 'immediately', 'nonetheless', 'dont', 'fooled', 'screenshot', 'real', 'arcade', 'game', 'plugnplay', 'looking', 'buy', 'like', 'review', 'agree', 'everything', 'bar', 'play', 'real', 'game', 'save', 'money', 'frustration', 'disappointment', 'gas', 'money', 'return', 'game', 'next', 'day', 'trust', 'me', 'will'], ['fans', 'martin', 'clunes', 'would', 'better', 'skipping', 'one', 'profoundly', 'disappointed', 'turned', 'dirty', 'tricks', 'clunes', 'character', 'simply', 'come', 'across', 'lovable', 'rogue', 'like', 'perhaps', 'steve', 'martin', 'michael', 'caine', 'played', 'dirty', 'rotten', 'scoundrels', 'instead', 'came', 'across', 'sleazy', 'despicabledirty', 'rotten', 'scoundrelsgraphic', 'spoilersi', 'particularly', 'put', 'scene', 'clunes', 'character', 'talks', 'dirty', 'married', 'woman', 'wants', 'seduce', 'masturbates', 'front', 'front', 'oblivious', 'husband', 'somehow', 'managed', 'incredibly', 'unbelievable', 'revolting', 'instead', 'appealingly', 'raunchyend', 'graphic', 'spoilersi', 'hardly', 'object', 'risque', 'humor', 'loved', 'dangerous', 'liaisons', 'and', 'valmont', 'story', 'found', 'watching', 'scene', 'made', 'feel', 'bit', 'disgusted', 'rather', 'damage', 'positive', 'image', 'ive', 'got', 'clunes', 'doc', 'martin', 'bailed', 'film', 'would', 'recommend', 'people', 'skip', 'itdangerous', 'liaisonsvalmont']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtuhsSmqwpoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from gensim.models import Word2Vec\n",
        "#model_vec_train = Word2Vec(sentences=sentences_train_processed, size=200, window=5, min_count=20, workers=4, sg=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHIV_h1JyO6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "79204388-8094-4b92-d186-85efc861dc0f"
      },
      "source": [
        "'''\n",
        "words = list(model_vec_train.wv.vocab)\n",
        "print('Vocabulary size: %d' % len(words))\n",
        "filename = 'final_embedding_word2vec.txt'\n",
        "model_vec_train.wv.save_word2vec_format(filename, binary=False)\n",
        "del model_vec_train'\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwords = list(model_vec_train.wv.vocab)\\nprint('Vocabulary size: %d' % len(words))\\nfilename = 'final_embedding_word2vec.txt'\\nmodel_vec_train.wv.save_word2vec_format(filename, binary=False)\\ndel model_vec_train'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Pxm1r7HVHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_train=[]\n",
        "for i in sentences_train_processed:\n",
        "  token=' '.join(i)\n",
        "  document_train.append(token)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD8PLl1OIIm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "01c975e9-8140-4364-d1c0-8679981a5c10"
      },
      "source": [
        "print(document_train[:1])\n",
        "print(document_train[1:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['great family camping great family camping trips afford weight batteries durable piece equipment decent battery life kids afraid dark want light tent good product']\n",
            "['defective pulling tabs would like relate recent experience ding king promise update new developments occurmy order arrived read instructions attempted repair glue would bond pulling tab called provided support number told perhaps used much glue tried using less glue made difference decided return product told potters gifts since product longer like new condition ineligible refund anger called support number let know intended let amazon community know products poor performance unjust treatment time told batch ding king product sent defective plastic adhere pulling tabs gave number 800 4716123 call remedy called sending pulling tabsi post receive them']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6kWMNoU0RVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import asarray,zeros\n",
        "def load_embedding(filename):\n",
        "\t# load embedding into memory\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()[1:]\n",
        "\tfile.close()\n",
        "\tembedding = dict()     # create a map of words to vectors\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\t# key is string word, value is numpy array for vector\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding\n",
        "def get_weight_matrix(embedding, vocab):\n",
        "\t# total vocabulary size plus 0 for unknown words\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\t# define weight matrix dimensions with all 0\n",
        "\tweight_matrix = zeros((vocab_size, 200))\n",
        "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
        "\tfor word, i in vocab.items():\n",
        "\t\tvector = embedding.get(word)\n",
        "\t\tif vector is not None:\n",
        "\t\t\tweight_matrix[i] = vector\n",
        "\treturn weight_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTUZuLs2UQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "5fcbeb1a-a9d0-45af-ff40-7a3fb042a804"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(document_train)\n",
        "encoded_docs = tokenizer.texts_to_sequences(document_train)\n",
        "print(encoded_docs[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[4, 238, 4372, 4, 238, 4372, 3333, 3672, 812, 701, 1298, 288, 2099, 602, 401, 70, 228, 1635, 553, 52, 267, 4296, 5, 25], [1145, 3015, 5949, 7, 3, 1547, 1295, 370, 14445, 832, 2230, 1984, 30, 10638, 348619, 271, 527, 9, 799, 3340, 1651, 2766, 7, 1885, 3015, 6273, 431, 1420, 464, 544, 455, 562, 69, 16, 2766, 181, 160, 203, 2766, 51, 895, 505, 309, 25, 455, 17519, 2583, 91, 25, 475, 3, 30, 773, 78347, 946, 3100, 431, 464, 544, 260, 40, 1681, 260, 127, 2473, 40, 557, 177, 580, 24059, 2085, 12, 455, 4316, 14445, 832, 25, 535, 1145, 397, 13072, 3015, 5949, 280, 544, 5133, 348620, 536, 8503, 431, 2022, 3015, 228720, 1833, 1283, 140]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0njeGWz-K1ni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c777afb8-3b2a-4029-f83d-d6d533dda0c6"
      },
      "source": [
        "print(tokenizer.word_index['bad'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hp6a__g6iDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "ff033dee-fb26-40d3-86a9-b78236d3c603"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_length = max([len(s) for s in encoded_docs])\n",
        "print(max_length)\n",
        "X_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(X_train.shape)\n",
        "print(X_train[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157\n",
            "(1338341, 157)\n",
            "[[     4    238   4372      4    238   4372   3333   3672    812    701\n",
            "    1298    288   2099    602    401     70    228   1635    553     52\n",
            "     267   4296      5     25      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0]\n",
            " [  1145   3015   5949      7      3   1547   1295    370  14445    832\n",
            "    2230   1984     30  10638 348619    271    527      9    799   3340\n",
            "    1651   2766      7   1885   3015   6273    431   1420    464    544\n",
            "     455    562     69     16   2766    181    160    203   2766     51\n",
            "     895    505    309     25    455  17519   2583     91     25    475\n",
            "       3     30    773  78347    946   3100    431    464    544    260\n",
            "      40   1681    260    127   2473     40    557    177    580  24059\n",
            "    2085     12    455   4316  14445    832     25    535   1145    397\n",
            "   13072   3015   5949    280    544   5133 348620    536   8503    431\n",
            "    2022   3015 228720   1833   1283    140      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH4z1-kvLn3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_test=[]\n",
        "for i in sentences_test_processed:\n",
        "  token=' '.join(i)\n",
        "  document_test.append(token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M90Aq2-XMsQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "a7727383-7d55-49ac-804b-2a33929d3f6e"
      },
      "source": [
        "encoded_docs = tokenizer.texts_to_sequences(document_test)\n",
        "#max_length = int(sum([len(s.split()) for s in document_train])/len(document_train))\n",
        "X_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(X_test.shape)\n",
        "print(X_test[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(446114, 157)\n",
            "[[    83   1293    840     96    282    632   2313   1577     34   2849\n",
            "    3168   3387  10902  12671   1340     28   1839   5973   4088    848\n",
            "    1419   4947     75   1032  81434   5570  76593  64417 578855 269046\n",
            "   15243  55123  11445 202751 171862    458 137880  19946  55411      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0]\n",
            " [    74      5    589    462  42954     13     33    212   1163      8\n",
            "      46      9    208    815     24   1187  23152      5      8     52\n",
            "     934    114     57      8   4272    379     12      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5401FyQkg2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "416eb939-9094-43e7-968d-6d818e083de2"
      },
      "source": [
        "print(type(tokenizer.word_index))\n",
        "print(len(tokenizer.word_index)+1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "1135192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1NU61ybkEsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtBXj3BCcokX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "from numpy import asarray,zeros\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "raw_embedding = load_embedding(r'/content/drive/My Drive/final_embedding_word2vec.txt')\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)  \n",
        "print(embedding_vectors.shape)                                                    #EMBEDDING LAYER\n",
        "print(embedding_vectors[1:3])\n",
        "embedding_layer = Embedding(vocab_size, 200,weights=[embedding_vectors] ,input_length=max_length, trainable=False)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcy51DICSKkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding,GlobalMaxPooling1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "#model.add(Conv1D(filters=200 ,kernel_size=2, activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=200, kernel_size=2, activation='relu',padding='valid'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jObWVopYBpBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=1)\n",
        "\n",
        "history=model.fit(X_train, y_train,validation_split=.25 ,epochs=100,batch_size=32, verbose=1,callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz3t_0zxRlz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=['good best excellent ','bad worse worst','dog good great bad ']\n",
        "encoded_a = tokenizer.texts_to_sequences(a)\n",
        "#max_length = int(sum([len(s.split()) for s in document_train])/len(document_train))\n",
        "a_test = pad_sequences(encoded_a, maxlen=max_length, padding='post')\n",
        "print(model.predict(a_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKl8q204OTrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft-I5dA5IwQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model is being evaluated for training dataset \n",
        "loss, acc = model.evaluate(X_train, y_train, verbose=1)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "#model is being evaluated for testing datset\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT9xYrzV9Tf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=['good best excellent ','bad worse worst','dog good great bad ']\n",
        "a=int(input(\"Enter the number of reviews:\"))\n",
        "s=[]\n",
        "for i in range(a):\n",
        "  s.append(input(\"Enter review->:\"))\n",
        "encoded_s = tokenizer.texts_to_sequences(s)\n",
        "#max_length = int(sum([len(s.split()) for s in document_train])/len(document_train))\n",
        "s_test = pad_sequences(encoded_s, maxlen=max_length, padding='post')\n",
        "for i in range(a):\n",
        "  if model.predict(s_test[i:i+1])>0.5:\n",
        "    print(\"Product recommended!\")\n",
        "  else:\n",
        "    print(\"Product is not recommended\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O185gtK-AgCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plt.plot(history.history['accuracy'])\n",
        "#plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}